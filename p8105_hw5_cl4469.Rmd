---
title: "p8105_hw5_cl4469"
author: "Chen Liang"
date: "2023-11-12"
output: github_document
---

```{r}
library(tidyverse)
library(patchwork)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "95%"
)

theme_set(theme_minimal() + theme(legend.position = 'bottom'))

options(
  ggplot2.continuous.colour = 'viridis',
  ggplot2.continuous.fill = 'viridis'
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1
```{r, load data}
homicide_df = read.csv("hw5_data/homicide-data.csv")
```
This dataset contains `r nrow(homicide_df)` rows and `r ncol(homicide_df)` columns, with each row resprenting a single event occurred. Variables include id, reported-date, victims'last name,race, and sex. It contains the location of the killing (city, state, latitude, longitude) and whether an arrest was made. Moreover, there are some missing values in `lat` and `lon` columns and unknown information in `victim_last`,`victim_first`,`victim_age`,`victim_sex`, and `victim_race`.

```{r}
hom_summary = homicide_df|>
  mutate(
    city_state = str_c(city, state, sep = ","),
    resolved = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved",
    )) |>
  group_by(city_state) |>
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolved == "unsolved"),
    home_solved = sum(resolved == "solved")
  )

hom_summary
```

```{r,prop test for Baltimore,MD}
res_tidy = prop.test(
  hom_summary |>filter(city_state == "Baltimore,MD") |> pull(hom_unsolved), 
  hom_summary |>filter(city_state == "Baltimore,MD") |> pull(hom_total)) |>
  broom::tidy(res_tidy)

save(res_tidy, file = "result/prop_test_MD.RData")
```

Pull the estimated proportion and confidence intervals from the resulting tidy dataframe
```{r}
estimated_prop=pull(res_tidy, estimate)
ci=paste("(", pull(res_tidy, conf.low),",", pull(res_tidy, conf.high),")")
```

```{r,make a plot}
results_df = 
  hom_summary %>% 
  mutate(
    prop_tests = map2(.x = hom_unsolved, .y = hom_total, ~prop.test(x = .x, n = .y)),
    tidy_tests = map(.x = prop_tests, ~broom::tidy(.x))
  ) %>% 
  select(-prop_tests) %>% 
  unnest(tidy_tests) %>% 
  select(city_state, estimate, conf.low, conf.high)
```

```{r}
results_df %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

# Problem 2
First, we create a dataframe and read in data for each subject.
```{r}
file_name =
  list.files("hw5_data/data/", full.names = TRUE)

prob2_df= file_name |>
  map_dfr(read_csv)|>
  bind_cols(name = file_name)
```

Next, we tidy the result.
```{r}
tidy_prob2_df =
  prob2_df|> 
  mutate(arm = 
           case_when(
             str_detect(name, "con") ~ "control",
             str_detect(name, "exp") ~ "experimental"),
         id = str_sub(name, 20, 21)
         ) |>
  pivot_longer(week_1:week_8,
               names_to = "week",
               values_to = "observations",
               names_prefix = "week_") |> 
  mutate(week = as.numeric(week))|>
  dplyr::select(id, arm, week, observations)
```

Make a spaghetti plot showing observations on each subject over time.
```{r}
tidy_prob2_df |>
  ggplot(aes(x = week, y = observations, color = id)) +
  geom_point()+
  geom_line() +
  facet_grid(~arm) +
  geom_smooth(aes(group = 1), se = FALSE, color = "red") +
  labs(x = 'Week', y = 'Observations', title = 'Spaghetti Plot of Observations on Each Subject Over Time')

ggsave("result/Spaghetti Plot of Observations on Each Subject Over Time.pdf")
```
Comment:
From the spaghetti plot, the data values of the experimental group seem to be generally greater than those of the control group. We also observe a gentle variation in values over time within the control arm, whereas the experimental arm consistently demonstrates higher values than those in the control arm, with a tendency to increase over time.

# Problem 3
Fix n=30, σ=5, and mu=0
```{r}
t_test = function(mu, n = 30, sigma = 5) {
  
  sim_data = tibble(
    x = rnorm(n, mean = mu, sd = sigma)
    )
  
  sim_result = broom::tidy(t.test(sim_data))
  sim_result |> 
    dplyr::select(estimate, p.value)
}

sim_results_df = 
  expand_grid(
    mu = 0,
    iter = 1:5000
    )|>
  mutate(
    estimate_df = map(mu, t_test)
    ) |> 
  unnest(estimate_df) |>
  rename("p_value" = "p.value")

sim_results_df
```

Repeat the above for mu=1,2,3,4,5,6,
```{r}
new_results_df = 
  expand_grid(
    mu = c(1,2,3,4,5,6),
    iter = 1:5000
  ) |> 
  mutate(
    estimate_df = map(mu, t_test)
  ) |> 
  unnest(estimate_df) |>
  rename("p_value" = "p.value")

```


Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of μ on the x axis
```{r}
all_result_df= bind_rows(sim_results_df, new_results_df)

power_df =
  all_result_df |> 
  mutate(reject = case_when(
    p_value > 0.05 ~ FALSE,
    p_value < 0.05 ~ TRUE
  )) |> 
  group_by(mu) |> 
  summarise(count = sum(reject)) |> 
  mutate(prop_rejext = count / 5000)

power_df |> 
  ggplot(aes(x = mu, y = prop_rejext)) +
  geom_point() +
  geom_line() +
  scale_x_continuous(breaks = seq(0,6))+
  labs(
       x = "True Value of Mu", 
       y = "Power of the Test",
       title = "Power of the Test vs. True Value of Mu")
```
Comment:
From this plot, we observe that the association between effect size and power is positive. The power of the test increases as the true value of mu increases, which means the power increases as the effect size increases, and gradually approaches 1.


Make a plot showing the average estimate of μ̂  on the y axis and the true value of μ on the x axis
```{r}
mean_esti_df=
  all_result_df |>
  group_by(mu) |>
  summarize(average_estimate = mean(estimate))

mean_esti_df|>
  ggplot(aes(x = mu, y = average_estimate)) + 
  geom_point() + 
  geom_line() +
  scale_x_continuous(breaks = seq(0,6,1)) +
  scale_y_continuous(breaks = seq(0,6,1)) +
  labs(x = "True Value of Mu", y = "Average Estimate of Mu", title = "Average Estimate of Mu vs. True Value of Mu")
```
Comment: From this plot, we can see that the average of estimated means across all tests is approximately equal to the true value of mean.


Make a second plot (or overlay on the first) the average estimate of μ̂  only in samples for which the null was rejected on the y axis and the true value of μ on the x axis. 
```{r}
rej_esti_df=
  all_result_df |>
  filter(p_value <= 0.05) |>
  group_by(mu) |>
  summarize(average_estimate = mean(estimate)) 

ggplot() +
  geom_line(data = mean_esti_df, aes(x = mu, y = average_estimate, color = "mean_esti_df")) +
  geom_point(data = mean_esti_df, aes(x = mu, y = average_estimate, color = "mean_esti_df")) +
  geom_line(data = rej_esti_df, aes(x = mu, y = average_estimate, color = "rej_esti_df")) +
  geom_point(data = rej_esti_df, aes(x = mu, y = average_estimate, color = "rej_esti_df")) +
  scale_x_continuous(breaks = seq(0,6,1)) +
  scale_y_continuous(breaks = seq(0,6,1)) +
  scale_color_manual(values = c("mean_esti_df" = "blue", "rej_esti_df" = "red"))
  labs(x = "True Value of Mu",
       y = "Average Estimate of Mu",
       title = "Mean Estimates and Rejections VS True Value of Mu")
```
Comment:The mean estimate of mu is greater in the rejected null dataset than it in all of the data, when the true mu is between 0 and 4; however, this difference becomes considerably less pronounced when the true mu approaches 4,5,6. Overall, the sample average of mu across tests for which the null is rejected is not approximately equal to the true value of mu, and this is because the null will be rejected under the confidence interval at an 0.05 significance level, meaning there is a 5% probability that the null hypothesis is rejected when it is actually true.